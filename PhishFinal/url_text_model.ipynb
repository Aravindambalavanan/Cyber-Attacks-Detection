{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from url_preprocessing.url_preprocessor import URLPreprocessor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/text_model_data.csv')\n",
    "df = df.sample(frac=1).reset_index(drop=True)  \n",
    "X = df['url']\n",
    "y = df['status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    \"Logistic Regression\": {'solver': ['liblinear'], 'penalty': ['l1', 'l2'], 'C': [0.01, 0.1, 1.0, 10, 20]},\n",
    "    \"Multinomial Naive Bayes\": {'alpha': [0.1, 1.0, 10]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(name, pipeline, X_train, X_test, y_train, y_test):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    best_classifier = pipeline.named_steps['classifier'].best_estimator_\n",
    "    print(f\"GridSearchCV results for {name}:\")\n",
    "    print(\"Best parameters found:\")\n",
    "    print(best_classifier.get_params())\n",
    "    y_train_pred = pipeline.predict(X_train)\n",
    "    y_test_pred = pipeline.predict(X_test)\n",
    "    acc_train = metrics.accuracy_score(y_train, y_train_pred)\n",
    "    acc_test = metrics.accuracy_score(y_test, y_test_pred)\n",
    "    f1_train = metrics.f1_score(y_train, y_train_pred)\n",
    "    f1_test = metrics.f1_score(y_test, y_test_pred)\n",
    "    recall_train = metrics.recall_score(y_train, y_train_pred)\n",
    "    recall_test = metrics.recall_score(y_test, y_test_pred)\n",
    "    precision_train = metrics.precision_score(y_train, y_train_pred)\n",
    "    precision_test = metrics.precision_score(y_test, y_test_pred)\n",
    "    return acc_train, acc_test, f1_train, f1_test, recall_train, recall_test, precision_train, precision_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_results(ML_Model, accuracy_train, accuracy_test, f1_score_train, f1_score_test, recall_train, recall_test, precision_train, precision_test):\n",
    "    results = {\"ML Model\": ML_Model, \"Accuracy (Train)\": accuracy_train, \"Accuracy (Test)\": accuracy_test, \"F1 Score (Train)\": f1_score_train, \"F1 Score (Test)\": f1_score_test, \"Recall (Train)\": recall_train, \"Recall (Test)\": recall_test, \"Precision (Train)\": precision_train, \"Precision (Test)\": precision_test}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV results for Logistic Regression:\n",
      "Best parameters found:\n",
      "{'C': 20, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Phishing       0.92      0.97      0.94     78899\n",
      "  Legitimate       0.97      0.92      0.95     85503\n",
      "\n",
      "    accuracy                           0.94    164402\n",
      "   macro avg       0.95      0.95      0.94    164402\n",
      "weighted avg       0.95      0.94      0.94    164402\n",
      "\n",
      "GridSearchCV results for Multinomial Naive Bayes:\n",
      "Best parameters found:\n",
      "{'alpha': 0.1, 'class_prior': None, 'fit_prior': True, 'force_alpha': 'warn'}\n",
      "Classification Report for Multinomial Naive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Phishing       0.97      0.89      0.93     78899\n",
      "  Legitimate       0.90      0.98      0.94     85503\n",
      "\n",
      "    accuracy                           0.93    164402\n",
      "   macro avg       0.94      0.93      0.93    164402\n",
      "weighted avg       0.94      0.93      0.93    164402\n",
      "\n",
      "Best performing model saved as 'text_model.joblib'. Classifier: Logistic Regression, with f1-Score: 0.9450982281591404\n",
      "                  ML Model  Accuracy (Train)  Accuracy (Test)  \\\n",
      "0      Logistic Regression          0.999790         0.944551   \n",
      "1  Multinomial Naive Bayes          0.997445         0.932702   \n",
      "\n",
      "   F1 Score (Train)  F1 Score (Test)  Recall (Train)  Recall (Test)  \\\n",
      "0          0.999798         0.945098        0.999739       0.917664   \n",
      "1          0.997541         0.937872        0.997778       0.976691   \n",
      "\n",
      "   Precision (Train)  Precision (Test)  \n",
      "0           0.999857          0.974224  \n",
      "1           0.997305          0.902021  \n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    classifiers = {\n",
    "        \"Logistic Regression\": Pipeline([\n",
    "            ('preprocessor', URLPreprocessor()),\n",
    "            ('vectorizer', CountVectorizer(tokenizer=None, stop_words=None, lowercase=False, ngram_range=(1, 2))),\n",
    "            ('classifier', GridSearchCV(LogisticRegression(), param_grids[\"Logistic Regression\"], cv=5, scoring='f1'))\n",
    "        ]),\n",
    "        \"Multinomial Naive Bayes\": Pipeline([\n",
    "            ('preprocessor', URLPreprocessor()),\n",
    "            ('vectorizer', CountVectorizer(tokenizer=None, stop_words=None, lowercase=False, ngram_range=(1, 2))),\n",
    "            ('classifier', GridSearchCV(MultinomialNB(), param_grids[\"Multinomial Naive Bayes\"], cv=5, n_jobs=-1, scoring='f1'))\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    best_model = None\n",
    "    best_f1_score = 0\n",
    "    best_model_name = \"\"\n",
    "    \n",
    "    results = []\n",
    "    for name, model in classifiers.items():\n",
    "        acc_train, acc_test, f1_train, f1_test, recall_train, recall_test, precision_train, precision_test = train_evaluate_model(name, model, X_train, X_test, y_train, y_test)\n",
    "        if f1_test > best_f1_score:\n",
    "            best_f1_score = f1_test\n",
    "            best_model = model\n",
    "            best_model_name = name\n",
    "        results.append(store_results(name, acc_train, acc_test, f1_train, f1_test, recall_train, recall_test, precision_train, precision_test))\n",
    "\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        print(f\"Classification Report for {name}:\")\n",
    "        print(metrics.classification_report(y_test, y_test_pred, target_names=('Phishing', 'Legitimate')))\n",
    "\n",
    "    if best_model is not None:\n",
    "        joblib.dump(best_model, 'text_model.joblib')\n",
    "        print(f\"Best performing model saved as 'text_model.joblib'. Classifier: {best_model_name}, with f1-Score: {best_f1_score}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    sorted_results = results_df.sort_values(by=['Accuracy (Test)', 'F1 Score (Test)'], ascending=False).reset_index(drop=True)\n",
    "    print(sorted_results)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
